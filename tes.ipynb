{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867eb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# improved_crawler.py\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiosqlite\n",
    "import logging\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import time\n",
    "import csv\n",
    "import signal\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import config\n",
    "\n",
    "class ImprovedURLCrawler:\n",
    "    def __init__(self):\n",
    "        self.config = config\n",
    "        self.setup_logging()\n",
    "        self.setup_signal_handlers()\n",
    "        \n",
    "        self.stats = {\n",
    "            'queued': 0, 'processed': 0, 'saved': 0, \n",
    "            'failed': 0, 'robots_denied': 0, 'duplicates': 0\n",
    "        }\n",
    "        \n",
    "        self.domain_counters = {}\n",
    "        self.url_cache = set()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('improved_crawler.log', encoding='utf-8'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def setup_signal_handlers(self):\n",
    "        def signal_handler(sig, frame):\n",
    "            self.logger.info(\"Received shutdown signal. Saving state...\")\n",
    "            asyncio.create_task(self.shutdown())\n",
    "        signal.signal(signal.SIGINT, signal_handler)\n",
    "        signal.signal(signal.SIGTERM, signal_handler)\n",
    "    \n",
    "    async def shutdown(self):\n",
    "        self.logger.info(\"Shutting down crawler gracefully...\")\n",
    "        if hasattr(self, 'session'):\n",
    "            await self.session.close()\n",
    "        if hasattr(self, 'db'):\n",
    "            await self.db.close()\n",
    "        sys.exit(0)\n",
    "    \n",
    "    async def init_database(self):\n",
    "        self.db = await aiosqlite.connect('crawler_queue.db')\n",
    "        await self.db.executescript(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS crawler_queue (\n",
    "                url TEXT PRIMARY KEY,\n",
    "                status TEXT CHECK(status IN ('queued', 'processing', 'completed', 'failed')),\n",
    "                depth INTEGER DEFAULT 0,\n",
    "                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            );\n",
    "            CREATE TABLE IF NOT EXISTS domain_delays (\n",
    "                domain TEXT PRIMARY KEY,\n",
    "                last_request TIMESTAMP,\n",
    "                delay_seconds REAL DEFAULT 1.0\n",
    "            );\n",
    "            CREATE TABLE IF NOT EXISTS robots_rules (\n",
    "                domain TEXT PRIMARY KEY,\n",
    "                rules TEXT,\n",
    "                last_fetched TIMESTAMP\n",
    "            );\n",
    "        \"\"\")\n",
    "        await self.db.commit()\n",
    "        await self.load_initial_seeds()\n",
    "    \n",
    "    async def load_initial_seeds(self):\n",
    "        for seed in self.config.SEEDS:\n",
    "            normalized = self.normalize_url(seed)\n",
    "            await self.add_url_to_queue(normalized, depth=0)\n",
    "        self.logger.info(f\"Loaded {len(self.config.SEEDS)} seed URLs\")\n",
    "    \n",
    "    def normalize_url(self, url, base_url=None):\n",
    "        if base_url:\n",
    "            url = urljoin(base_url, url)\n",
    "        \n",
    "        # Remove fragments and normalize\n",
    "        url = url.split('#')[0]\n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        # Normalize scheme and hostname\n",
    "        scheme = parsed.scheme.lower()\n",
    "        netloc = parsed.netloc.lower()\n",
    "        \n",
    "        # Remove www prefix and trailing slashes\n",
    "        if netloc.startswith('www.'):\n",
    "            netloc = netloc[4:]\n",
    "        \n",
    "        path = parsed.path.rstrip('/')\n",
    "        if not path:\n",
    "            path = '/'\n",
    "            \n",
    "        normalized = f\"{scheme}://{netloc}{path}\"\n",
    "        if parsed.query:\n",
    "            normalized += f\"?{parsed.query}\"\n",
    "            \n",
    "        return normalized\n",
    "    \n",
    "    def get_domain(self, url):\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc.lower().replace('www.', '')\n",
    "    \n",
    "    def should_crawl_url(self, url):\n",
    "        domain = self.get_domain(url)\n",
    "        \n",
    "        # Check allowed domains\n",
    "        if domain not in [d.replace('www.', '') for d in self.config.ALLOWED_DOMAINS]:\n",
    "            return False\n",
    "        \n",
    "        # Check keyword filter\n",
    "        url_lower = url.lower()\n",
    "        if any(keyword in url_lower for keyword in self.config.KEYWORD_FILTER):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    async def get_robots_parser(self, domain):\n",
    "        cursor = await self.db.execute(\n",
    "            \"SELECT rules, last_fetched FROM robots_rules WHERE domain = ?\", (domain,)\n",
    "        )\n",
    "        result = await cursor.fetchone()\n",
    "        \n",
    "        if result and (time.time() - result[1]) < 86400:\n",
    "            rules_text = result[0]\n",
    "        else:\n",
    "            try:\n",
    "                async with self.session.get(f\"https://{domain}/robots.txt\") as response:\n",
    "                    if response.status == 200:\n",
    "                        rules_text = await response.text()\n",
    "                    else:\n",
    "                        rules_text = \"\"\n",
    "                \n",
    "                await self.db.execute(\n",
    "                    \"INSERT OR REPLACE INTO robots_rules (domain, rules, last_fetched) VALUES (?, ?, ?)\",\n",
    "                    (domain, rules_text, time.time())\n",
    "                )\n",
    "                await self.db.commit()\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Could not fetch robots.txt for {domain}: {e}\")\n",
    "                rules_text = \"\"\n",
    "        \n",
    "        parser = RobotFileParser()\n",
    "        parser.parse(rules_text.splitlines())\n",
    "        return parser\n",
    "    \n",
    "    async def can_fetch(self, url):\n",
    "        domain = self.get_domain(url)\n",
    "        parser = await self.get_robots_parser(domain)\n",
    "        return parser.can_fetch(\"*\", url)\n",
    "    \n",
    "    async def add_url_to_queue(self, url, depth=0):\n",
    "        if url in self.url_cache:\n",
    "            self.stats['duplicates'] += 1\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            await self.db.execute(\n",
    "                \"INSERT OR IGNORE INTO crawler_queue (url, status, depth) VALUES (?, 'queued', ?)\",\n",
    "                (url, depth)\n",
    "            )\n",
    "            await self.db.commit()\n",
    "            self.url_cache.add(url)\n",
    "            self.stats['queued'] += 1\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error adding URL {url} to queue: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def apply_domain_delay(self, domain):\n",
    "        cursor = await self.db.execute(\n",
    "            \"SELECT last_request FROM domain_delays WHERE domain = ?\", (domain,)\n",
    "        )\n",
    "        result = await cursor.fetchone()\n",
    "        \n",
    "        if result:\n",
    "            last_request = result[0]\n",
    "            elapsed = time.time() - last_request\n",
    "            if elapsed < self.config.PER_DOMAIN_DELAY:\n",
    "                await asyncio.sleep(self.config.PER_DOMAIN_DELAY - elapsed)\n",
    "        \n",
    "        await self.db.execute(\n",
    "            \"INSERT OR REPLACE INTO domain_delays (domain, last_request) VALUES (?, ?)\",\n",
    "            (domain, time.time())\n",
    "        )\n",
    "        await self.db.commit()\n",
    "    \n",
    "    async def process_url(self, url, depth):\n",
    "        try:\n",
    "            domain = self.get_domain(url)\n",
    "            \n",
    "            # Check robots.txt\n",
    "            if not await self.can_fetch(url):\n",
    "                self.logger.info(f\"Robots.txt denied: {url}\")\n",
    "                self.stats['robots_denied'] += 1\n",
    "                await self.mark_url_completed(url, 'failed')\n",
    "                return\n",
    "            \n",
    "            # Apply domain delay\n",
    "            await self.apply_domain_delay(domain)\n",
    "            \n",
    "            # Fetch URL with retries\n",
    "            for retry in range(self.config.MAX_RETRIES):\n",
    "                try:\n",
    "                    async with self.session.get(\n",
    "                        url, \n",
    "                        timeout=aiohttp.ClientTimeout(total=self.config.REQUEST_TIMEOUT),\n",
    "                        headers={'User-Agent': 'Mozilla/5.0 (compatible; TechScopeBot/1.0)'}\n",
    "                    ) as response:\n",
    "                        \n",
    "                        if response.status != 200:\n",
    "                            continue\n",
    "                            \n",
    "                        html = await response.text()\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    if retry == self.config.MAX_RETRIES - 1:\n",
    "                        raise e\n",
    "                    await asyncio.sleep(2 ** retry)\n",
    "            else:\n",
    "                return\n",
    "            \n",
    "            # Parse and extract links\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            links = set()\n",
    "            \n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith(('http', '//', '/')):\n",
    "                    normalized = self.normalize_url(href, url)\n",
    "                    if self.should_crawl_url(normalized):\n",
    "                        links.add(normalized)\n",
    "            \n",
    "            # Add new links to queue\n",
    "            for link in links:\n",
    "                if self.stats['saved'] >= self.config.MAX_URLS:\n",
    "                    break\n",
    "                await self.add_url_to_queue(link, depth + 1)\n",
    "            \n",
    "            # Save to CSV if it's a product page\n",
    "            if self.is_product_page(soup, url):\n",
    "                await self.save_url_to_csv(url)\n",
    "            \n",
    "            await self.mark_url_completed(url, 'completed')\n",
    "            \n",
    "            if self.stats['processed'] % 100 == 0:\n",
    "                self.logger.info(f\"Progress: {self.stats['processed']} processed, {self.stats['saved']} saved\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {url}: {e}\")\n",
    "            await self.mark_url_completed(url, 'failed')\n",
    "            self.stats['failed'] += 1\n",
    "    \n",
    "    def is_product_page(self, soup, url):\n",
    "        \"\"\"Detect if page is a product page\"\"\"\n",
    "        # Check for common product indicators\n",
    "        product_indicators = [\n",
    "            soup.find('meta', property='og:type', content='product'),\n",
    "            soup.find('div', class_=re.compile(r'product', re.I)),\n",
    "            soup.find('div', class_=re.compile(r'item', re.I)),\n",
    "            soup.find('span', class_=re.compile(r'price', re.I)),\n",
    "            soup.find('button', class_=re.compile(r'buy|cart|beli', re.I)),\n",
    "        ]\n",
    "        \n",
    "        return any(indicator for indicator in product_indicators if indicator)\n",
    "    \n",
    "    async def save_url_to_csv(self, url):\n",
    "        url_id = f\"07-{self.stats['saved'] + 1:07d}\"\n",
    "        timestamp = datetime.now(timezone.utc).isoformat()\n",
    "        \n",
    "        with open('collected_urls.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([url_id, url, timestamp])\n",
    "        \n",
    "        self.stats['saved'] += 1\n",
    "    \n",
    "    async def mark_url_completed(self, url, status):\n",
    "        await self.db.execute(\n",
    "            \"UPDATE crawler_queue SET status = ? WHERE url = ?\", (status, url)\n",
    "        )\n",
    "        await self.db.commit()\n",
    "        self.stats['processed'] += 1\n",
    "    \n",
    "    async def get_next_url(self):\n",
    "        cursor = await self.db.execute(\n",
    "            \"SELECT url, depth FROM crawler_queue WHERE status = 'queued' ORDER BY depth LIMIT 1\"\n",
    "        )\n",
    "        result = await cursor.fetchone()\n",
    "        \n",
    "        if result:\n",
    "            await self.db.execute(\n",
    "                \"UPDATE crawler_queue SET status = 'processing' WHERE url = ?\", (result[0],)\n",
    "            )\n",
    "            await self.db.commit()\n",
    "            return result\n",
    "        return None\n",
    "    \n",
    "    async def run(self):\n",
    "        self.logger.info(\"Starting improved URL crawler...\")\n",
    "        \n",
    "        await self.init_database()\n",
    "        \n",
    "        # Initialize CSV file\n",
    "        with open('collected_urls.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['id', 'source_url', 'found_at'])\n",
    "        \n",
    "        # Setup HTTP session\n",
    "        connector = aiohttp.TCPConnector(limit=self.config.MAX_CONCURRENT_TASKS)\n",
    "        self.session = aiohttp.ClientSession(connector=connector)\n",
    "        \n",
    "        # Main processing loop\n",
    "        tasks = set()\n",
    "        while self.stats['saved'] < self.config.MAX_URLS:\n",
    "            while len(tasks) < self.config.MAX_CONCURRENT_TASKS:\n",
    "                next_url = await self.get_next_url()\n",
    "                if not next_url:\n",
    "                    if not tasks:\n",
    "                        break\n",
    "                    await asyncio.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                url, depth = next_url\n",
    "                task = asyncio.create_task(self.process_url(url, depth))\n",
    "                tasks.add(task)\n",
    "                task.add_done_callback(tasks.discard)\n",
    "            \n",
    "            await asyncio.sleep(0.1)\n",
    "        \n",
    "        # Cleanup\n",
    "        await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        await self.session.close()\n",
    "        await self.db.close()\n",
    "        \n",
    "        self.logger.info(f\"Crawling completed. Saved {self.stats['saved']} URLs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
