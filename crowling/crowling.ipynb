{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcd28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting aiosqlite\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: ipython in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (9.6.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting typing_extensions>=4.0 (from aiosqlite)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: decorator in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from stack_data->ipython) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/muliaandiki/Kuliah/PI/project/techscope/.venv/lib/python3.12/site-packages (from stack_data->ipython) (0.2.3)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;249;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install aiohttp aiosqlite beautifulsoup4 pandas ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff3012",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiohttp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maiohttp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maiosqlite\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'aiohttp'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiosqlite\n",
    "import logging\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "try:\n",
    "    from config import SEEDS, MAX_URLS, MAX_CONCURRENT_TASKS, PER_DOMAIN_DELAY, REQUEST_TIMEOUT, KEYWORD_FILTER, ALLOWED_DOMAINS\n",
    "    print(\"‚úÖ Config loaded successfully!\")\n",
    "    print(f\"üéØ Target: {MAX_URLS:,} URLs\")\n",
    "    print(f\"üåê Seeds: {len(SEEDS)} domains\")\n",
    "    print(f\"‚ö° Concurrency: {MAX_CONCURRENT_TASKS} tasks\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error loading config: {e}\")\n",
    "    print(\"üìù Pastikan file config.py ada di directory yang sama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb335bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Definisi Crawler Class yang Dioptimasi\n",
    "class TechScopeCrawler:\n",
    "    def __init__(self):\n",
    "        self.setup_logging()\n",
    "        self.stats = {\n",
    "            'queued': 0, 'processed': 0, 'saved': 0, \n",
    "            'failed': 0, 'robots_denied': 0, 'duplicates': 0,\n",
    "            'start_time': time.time()\n",
    "        }\n",
    "        self.url_cache = set()\n",
    "        self.session = None\n",
    "        self.db = None\n",
    "        self.is_running = True\n",
    "        self.processed_urls = set()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[logging.StreamHandler(sys.stdout)]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def normalize_url(self, url, base_url=None):\n",
    "        \"\"\"Normalize URL dengan efisien\"\"\"\n",
    "        try:\n",
    "            if base_url:\n",
    "                url = urljoin(base_url, url)\n",
    "            url = url.split('#')[0]\n",
    "            parsed = urlparse(url)\n",
    "            scheme = parsed.scheme.lower() or 'https'\n",
    "            netloc = parsed.netloc.lower().replace('www.', '')\n",
    "            path = parsed.path.rstrip('/') or '/'\n",
    "            normalized = f\"{scheme}://{netloc}{path}\"\n",
    "            if parsed.query:\n",
    "                normalized += f\"?{parsed.query}\"\n",
    "            return normalized\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def get_domain(self, url):\n",
    "        \"\"\"Extract domain dengan cepat\"\"\"\n",
    "        try:\n",
    "            return urlparse(url).netloc.lower().replace('www.', '')\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    \n",
    "    def should_crawl_url(self, url):\n",
    "        \"\"\"Optimized URL filtering\"\"\"\n",
    "        if not url or url in self.processed_urls:\n",
    "            return False\n",
    "            \n",
    "        domain = self.get_domain(url)\n",
    "        if not domain:\n",
    "            return False\n",
    "            \n",
    "        # Cek domain allowed\n",
    "        if not any(allowed_domain in domain for allowed_domain in ALLOWED_DOMAINS):\n",
    "            return False\n",
    "        \n",
    "        # Cek keyword filter\n",
    "        url_lower = url.lower()\n",
    "        return any(keyword in url_lower for keyword in KEYWORD_FILTER)\n",
    "    \n",
    "    async def init_database(self):\n",
    "        \"\"\"Initialize database dengan error handling\"\"\"\n",
    "        try:\n",
    "            self.db = await aiosqlite.connect('techscope_crawler.db', timeout=30)\n",
    "            \n",
    "            # Optimized schema\n",
    "            await self.db.executescript(\"\"\"\n",
    "                PRAGMA journal_mode=WAL;\n",
    "                PRAGMA synchronous=NORMAL;\n",
    "                PRAGMA cache_size=10000;\n",
    "                \n",
    "                CREATE TABLE IF NOT EXISTS crawler_queue (\n",
    "                    url TEXT PRIMARY KEY,\n",
    "                    status TEXT DEFAULT 'queued',\n",
    "                    depth INTEGER DEFAULT 0,\n",
    "                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "                \n",
    "                CREATE TABLE IF NOT EXISTS domain_delays (\n",
    "                    domain TEXT PRIMARY KEY,\n",
    "                    last_request TIMESTAMP DEFAULT 0,\n",
    "                    delay_seconds REAL DEFAULT 1.5\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_queue_status ON crawler_queue(status);\n",
    "                CREATE INDEX IF NOT EXISTS idx_queue_depth ON crawler_queue(depth);\n",
    "            \"\"\")\n",
    "            await self.db.commit()\n",
    "            \n",
    "            # Load seeds dengan batch processing\n",
    "            batch_size = 50\n",
    "            for i in range(0, len(SEEDS), batch_size):\n",
    "                batch = SEEDS[i:i + batch_size]\n",
    "                for seed in batch:\n",
    "                    normalized = self.normalize_url(seed)\n",
    "                    if normalized and self.should_crawl_url(normalized):\n",
    "                        await self.add_url_to_queue(normalized, depth=0)\n",
    "                \n",
    "            print(f\"üå± Loaded {len(SEEDS)} seeds, {self.stats['queued']} URLs queued\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database initialization failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def add_url_to_queue(self, url, depth=0):\n",
    "        \"\"\"Optimized URL queue addition\"\"\"\n",
    "        if url in self.url_cache or depth > 5:  # Limit depth\n",
    "            self.stats['duplicates'] += 1\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            await self.db.execute(\n",
    "                \"INSERT OR IGNORE INTO crawler_queue (url, status, depth) VALUES (?, 'queued', ?)\",\n",
    "                (url, depth)\n",
    "            )\n",
    "            self.url_cache.add(url)\n",
    "            self.stats['queued'] += 1\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    async def get_robots_parser(self, domain):\n",
    "        \"\"\"Cached robots.txt parser\"\"\"\n",
    "        try:\n",
    "            async with self.session.get(\n",
    "                f\"https://{domain}/robots.txt\", \n",
    "                timeout=10,\n",
    "                headers={'User-Agent': 'TechScopeBot/1.0'}\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    rules_text = await response.text()\n",
    "                else:\n",
    "                    rules_text = \"\"\n",
    "            \n",
    "            parser = RobotFileParser()\n",
    "            if rules_text:\n",
    "                parser.parse(rules_text.splitlines())\n",
    "            return parser\n",
    "        except Exception:\n",
    "            # Return permissive parser jika error\n",
    "            parser = RobotFileParser()\n",
    "            return parser\n",
    "    \n",
    "    async def can_fetch(self, url):\n",
    "        \"\"\"Fast robots.txt check\"\"\"\n",
    "        try:\n",
    "            domain = self.get_domain(url)\n",
    "            parser = await self.get_robots_parser(domain)\n",
    "            return parser.can_fetch(\"TechScopeBot\", url)\n",
    "        except Exception:\n",
    "            return True\n",
    "    \n",
    "    async def apply_domain_delay(self, domain):\n",
    "        \"\"\"Efficient domain rate limiting\"\"\"\n",
    "        try:\n",
    "            cursor = await self.db.execute(\n",
    "                \"SELECT last_request FROM domain_delays WHERE domain = ?\", \n",
    "                (domain,)\n",
    "            )\n",
    "            result = await cursor.fetchone()\n",
    "            \n",
    "            if result and result[0]:\n",
    "                elapsed = time.time() - result[0]\n",
    "                if elapsed < PER_DOMAIN_DELAY:\n",
    "                    await asyncio.sleep(PER_DOMAIN_DELAY - elapsed)\n",
    "            \n",
    "            await self.db.execute(\n",
    "                \"INSERT OR REPLACE INTO domain_delays (domain, last_request) VALUES (?, ?)\",\n",
    "                (domain, time.time())\n",
    "            )\n",
    "        except Exception:\n",
    "            pass  # Skip delay jika error\n",
    "    \n",
    "    async def process_url(self, url, depth):\n",
    "        \"\"\"Optimized URL processing\"\"\"\n",
    "        try:\n",
    "            self.processed_urls.add(url)\n",
    "            domain = self.get_domain(url)\n",
    "            \n",
    "            # Fast robots check\n",
    "            if not await self.can_fetch(url):\n",
    "                self.stats['robots_denied'] += 1\n",
    "                await self.mark_url_completed(url, 'failed')\n",
    "                return\n",
    "            \n",
    "            # Apply rate limiting\n",
    "            await self.apply_domain_delay(domain)\n",
    "            \n",
    "            # Fetch dengan timeout\n",
    "            try:\n",
    "                async with self.session.get(\n",
    "                    url, \n",
    "                    timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT),\n",
    "                    headers={\n",
    "                        'User-Agent': 'Mozilla/5.0 (compatible; TechScopeBot/1.0)',\n",
    "                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                        'Accept-Language': 'en-US,en;q=0.5',\n",
    "                    }\n",
    "                ) as response:\n",
    "                    \n",
    "                    if response.status != 200:\n",
    "                        await self.mark_url_completed(url, 'failed')\n",
    "                        self.stats['failed'] += 1\n",
    "                        return\n",
    "                    \n",
    "                    html = await response.text()\n",
    "                    \n",
    "            except asyncio.TimeoutError:\n",
    "                await self.mark_url_completed(url, 'failed')\n",
    "                self.stats['failed'] += 1\n",
    "                return\n",
    "            except Exception:\n",
    "                await self.mark_url_completed(url, 'failed')\n",
    "                self.stats['failed'] += 1\n",
    "                return\n",
    "            \n",
    "            # Fast HTML parsing dan link extraction\n",
    "            try:\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "                links = set()\n",
    "                \n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    href = link.get('href', '')\n",
    "                    if href.startswith(('http', '/', '//')):\n",
    "                        normalized = self.normalize_url(href, url)\n",
    "                        if normalized and self.should_crawl_url(normalized):\n",
    "                            links.add(normalized)\n",
    "                \n",
    "                # Batch add new links\n",
    "                for link in links:\n",
    "                    if self.stats['saved'] >= MAX_URLS:\n",
    "                        break\n",
    "                    await self.add_url_to_queue(link, depth + 1)\n",
    "                \n",
    "            except Exception:\n",
    "                links = set()\n",
    "            \n",
    "            # Save URL yang relevan\n",
    "            if self.is_relevant_content(soup, url):\n",
    "                await self.save_url_to_csv(url)\n",
    "            \n",
    "            await self.mark_url_completed(url, 'completed')\n",
    "            \n",
    "            # Progress reporting\n",
    "            if self.stats['saved'] % 100 == 0:\n",
    "                elapsed = time.time() - self.stats['start_time']\n",
    "                urls_per_sec = self.stats['processed'] / elapsed if elapsed > 0 else 0\n",
    "                print(f\"üìà Progress: {self.stats['saved']:,}/{MAX_URLS:,} | \"\n",
    "                      f\"Speed: {urls_per_sec:.1f} URLs/sec\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            await self.mark_url_completed(url, 'failed')\n",
    "            self.stats['failed'] += 1\n",
    "    \n",
    "    def is_relevant_content(self, soup, url):\n",
    "        \"\"\"Fast relevance checking\"\"\"\n",
    "        # Cek meta tags\n",
    "        meta_product = soup.find('meta', property='og:type')\n",
    "        if meta_product and 'product' in str(meta_product.get('content', '')).lower():\n",
    "            return True\n",
    "        \n",
    "        # Cek common product indicators\n",
    "        product_indicators = [\n",
    "            soup.find('meta', property='product:price:amount'),\n",
    "            soup.find('span', class_=re.compile(r'price', re.I)),\n",
    "            soup.find('button', class_=re.compile(r'buy|cart|beli', re.I)),\n",
    "        ]\n",
    "        \n",
    "        if any(indicator for indicator in product_indicators if indicator):\n",
    "            return True\n",
    "        \n",
    "        # Cek URL patterns\n",
    "        product_patterns = ['/product/', '/p/', '/item/', '/laptop/', '/smartphone/', '/tablet/']\n",
    "        if any(pattern in url.lower() for pattern in product_patterns):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    async def save_url_to_csv(self, url):\n",
    "        \"\"\"Fast CSV saving\"\"\"\n",
    "        try:\n",
    "            url_id = f\"07-{self.stats['saved'] + 1:07d}\"\n",
    "            timestamp = datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n",
    "            \n",
    "            with open('collected_urls.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([url_id, url, timestamp])\n",
    "            \n",
    "            self.stats['saved'] += 1\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    async def mark_url_completed(self, url, status):\n",
    "        \"\"\"Fast status update\"\"\"\n",
    "        try:\n",
    "            await self.db.execute(\n",
    "                \"UPDATE crawler_queue SET status = ? WHERE url = ?\",\n",
    "                (status, url)\n",
    "            )\n",
    "            self.stats['processed'] += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    async def get_next_batch(self, limit=10):\n",
    "        \"\"\"Batch processing untuk performance\"\"\"\n",
    "        try:\n",
    "            cursor = await self.db.execute(\n",
    "                \"SELECT url, depth FROM crawler_queue WHERE status = 'queued' ORDER BY depth ASC LIMIT ?\",\n",
    "                (limit,)\n",
    "            )\n",
    "            results = await cursor.fetchall()\n",
    "            \n",
    "            if results:\n",
    "                # Mark as processing\n",
    "                placeholders = ','.join('?' * len(results))\n",
    "                urls = [result[0] for result in results]\n",
    "                await self.db.execute(\n",
    "                    f\"UPDATE crawler_queue SET status = 'processing' WHERE url IN ({placeholders})\",\n",
    "                    urls\n",
    "                )\n",
    "                await self.db.commit()\n",
    "                \n",
    "            return results\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"Main crawling loop yang dioptimasi\"\"\"\n",
    "        print(\"üöÄ Starting TechScope Crawler...\")\n",
    "        print(f\"üéØ Target: {MAX_URLS:,} URLs\")\n",
    "        print(f\"‚ö° Concurrency: {MAX_CONCURRENT_TASKS} tasks\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Initialize\n",
    "            await self.init_database()\n",
    "            \n",
    "            # Initialize CSV\n",
    "            with open('collected_urls.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['id', 'source_url', 'found_at'])\n",
    "            \n",
    "            # Setup HTTP session dengan connection pooling\n",
    "            connector = aiohttp.TCPConnector(\n",
    "                limit=MAX_CONCURRENT_TASKS,\n",
    "                limit_per_host=3,\n",
    "                ttl_dns_cache=300\n",
    "            )\n",
    "            self.session = aiohttp.ClientSession(connector=connector)\n",
    "            \n",
    "            # Main processing loop\n",
    "            tasks = set()\n",
    "            batch_size = MAX_CONCURRENT_TASKS * 2\n",
    "            \n",
    "            while (self.stats['saved'] < MAX_URLS and \n",
    "                   self.is_running and \n",
    "                   self.stats['processed'] < (self.stats['queued'] + 1000)):\n",
    "                \n",
    "                # Fill task queue dengan batch processing\n",
    "                while len(tasks) < MAX_CONCURRENT_TASKS and self.is_running:\n",
    "                    batch = await self.get_next_batch(batch_size)\n",
    "                    if not batch:\n",
    "                        if not tasks:\n",
    "                            print(\"‚è≥ No more URLs in queue, waiting...\")\n",
    "                            await asyncio.sleep(2)\n",
    "                            continue\n",
    "                        break\n",
    "                    \n",
    "                    for url, depth in batch:\n",
    "                        if len(tasks) >= MAX_CONCURRENT_TASKS:\n",
    "                            break\n",
    "                        task = asyncio.create_task(self.process_url(url, depth))\n",
    "                        tasks.add(task)\n",
    "                        task.add_done_callback(tasks.discard)\n",
    "                \n",
    "                # Progress monitoring\n",
    "                if self.stats['processed'] % 50 == 0:\n",
    "                    elapsed = time.time() - self.stats['start_time']\n",
    "                    progress_pct = (self.stats['saved'] / MAX_URLS) * 100\n",
    "                    urls_per_sec = self.stats['processed'] / elapsed if elapsed > 0 else 0\n",
    "                    \n",
    "                    clear_output(wait=True)\n",
    "                    print(\"üìä REAL-TIME PROGRESS:\")\n",
    "                    print(f\"‚úÖ Saved: {self.stats['saved']:,} / {MAX_URLS:,} ({progress_pct:.1f}%)\")\n",
    "                    print(f\"üîÑ Processed: {self.stats['processed']:,}\")\n",
    "                    print(f\"‚ùå Failed: {self.stats['failed']:,}\")\n",
    "                    print(f\"üö´ Robots Denied: {self.stats['robots_denied']:,}\")\n",
    "                    print(f\"‚è≥ Active Tasks: {len(tasks)}\")\n",
    "                    print(f\"‚ö° Speed: {urls_per_sec:.1f} URLs/sec\")\n",
    "                    print(f\"‚è∞ Elapsed: {elapsed:.0f}s\")\n",
    "                    print(\"-\" * 50)\n",
    "                \n",
    "                await asyncio.sleep(0.1)\n",
    "            \n",
    "            # Wait for remaining tasks\n",
    "            if tasks:\n",
    "                print(f\"‚è≥ Waiting for {len(tasks)} remaining tasks...\")\n",
    "                await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"üéâ CRAWLING COMPLETED!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Crawling failed: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if self.session:\n",
    "                await self.session.close()\n",
    "            if self.db:\n",
    "                await self.db.commit()\n",
    "                await self.db.close()\n",
    "            \n",
    "            # Final stats\n",
    "            elapsed = time.time() - self.stats['start_time']\n",
    "            print(f\"üìà Final Stats:\")\n",
    "            print(f\"   ‚úÖ URLs Saved: {self.stats['saved']:,}\")\n",
    "            print(f\"   üîÑ Total Processed: {self.stats['processed']:,}\")\n",
    "            print(f\"   ‚ùå Failed: {self.stats['failed']:,}\")\n",
    "            print(f\"   üö´ Robots Denied: {self.stats['robots_denied']:,}\")\n",
    "            print(f\"   üîÅ Duplicates: {self.stats['duplicates']:,}\")\n",
    "            print(f\"   ‚è∞ Total Time: {elapsed:.1f}s\")\n",
    "            print(f\"   ‚ö° Average Speed: {self.stats['processed']/elapsed:.1f} URLs/sec\")\n",
    "\n",
    "print(\"‚úÖ TechScope Crawler class ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Jalankan Crawler\n",
    "async def main():\n",
    "    crawler = TechScopeCrawler()\n",
    "    await crawler.run()\n",
    "    return crawler.stats\n",
    "\n",
    "print(\"üîÑ Starting TechScope Crawling Process...\")\n",
    "print(\"‚ö†Ô∏è  This may take a while for 1,000,000 URLs\")\n",
    "print(\"üí° Press Ctrl+C to stop gracefully\")\n",
    "\n",
    "# Jalankan crawler\n",
    "final_stats = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Hasil dan Analisis\n",
    "def analyze_results():\n",
    "    \"\"\"Analyze crawling results\"\"\"\n",
    "    if not os.path.exists('collected_urls.csv'):\n",
    "        print(\"‚ùå No results file found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('collected_urls.csv')\n",
    "        print(f\"üìä RESULTS ANALYSIS:\")\n",
    "        print(f\"   üìÑ Total URLs Collected: {len(df):,}\")\n",
    "        \n",
    "        # Extract domains\n",
    "        df['domain'] = df['source_url'].apply(lambda x: urlparse(x).netloc)\n",
    "        \n",
    "        print(f\"   üåê Unique Domains: {df['domain'].nunique()}\")\n",
    "        print(f\"   üìÖ First URL: {df['found_at'].min()}\")\n",
    "        print(f\"   üìÖ Last URL: {df['found_at'].max()}\")\n",
    "        \n",
    "        # Top domains\n",
    "        print(f\"\\nüèÜ TOP DOMAINS:\")\n",
    "        top_domains = df['domain'].value_counts().head(10)\n",
    "        for domain, count in top_domains.items():\n",
    "            print(f\"   {domain}: {count:,} URLs\")\n",
    "        \n",
    "        # Sample URLs\n",
    "        print(f\"\\nüîç SAMPLE URLs:\")\n",
    "        for i, url in enumerate(df['source_url'].head(5).tolist()):\n",
    "            print(f\"   {i+1}. {url}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing results: {e}\")\n",
    "        return None\n",
    "\n",
    "results_df = analyze_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Export Summary Report\n",
    "def create_detailed_summary():\n",
    "    \"\"\"Create comprehensive summary report\"\"\"\n",
    "    summary = {\n",
    "        'crawling_session': {\n",
    "            'start_time': datetime.fromtimestamp(final_stats['start_time']).isoformat(),\n",
    "            'end_time': datetime.now().isoformat(),\n",
    "            'duration_seconds': time.time() - final_stats['start_time'],\n",
    "            'target_urls': MAX_URLS,\n",
    "            'achieved_urls': final_stats['saved']\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'urls_processed': final_stats['processed'],\n",
    "            'urls_saved': final_stats['saved'],\n",
    "            'success_rate': (final_stats['saved'] / final_stats['processed']) * 100 if final_stats['processed'] > 0 else 0,\n",
    "            'urls_per_second': final_stats['processed'] / (time.time() - final_stats['start_time']),\n",
    "            'failures': final_stats['failed'],\n",
    "            'robots_denied': final_stats['robots_denied'],\n",
    "            'duplicates_skipped': final_stats['duplicates']\n",
    "        },\n",
    "        'configuration': {\n",
    "            'max_concurrent_tasks': MAX_CONCURRENT_TASKS,\n",
    "            'domain_delay': PER_DOMAIN_DELAY,\n",
    "            'request_timeout': REQUEST_TIMEOUT,\n",
    "            'seeds_count': len(SEEDS),\n",
    "            'allowed_domains': len(ALLOWED_DOMAINS)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save summary\n",
    "    import json\n",
    "    with open('crawling_summary_detailed.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"üìã DETAILED SUMMARY:\")\n",
    "    print(f\"   üéØ Target: {summary['crawling_session']['target_urls']:,} URLs\")\n",
    "    print(f\"   ‚úÖ Achieved: {summary['crawling_session']['achieved_urls']:,} URLs\")\n",
    "    print(f\"   üìà Success Rate: {summary['performance_metrics']['success_rate']:.1f}%\")\n",
    "    print(f\"   ‚ö° Speed: {summary['performance_metrics']['urls_per_second']:.1f} URLs/sec\")\n",
    "    print(f\"   ‚è∞ Duration: {summary['crawling_session']['duration_seconds']:.0f} seconds\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "if 'final_stats' in locals():\n",
    "    detailed_summary = create_detailed_summary()\n",
    "else:\n",
    "    print(\"‚ùå Run Cell 3 first to get crawling results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
